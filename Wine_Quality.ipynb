{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset: Wine Quality**"
      ],
      "metadata": {
        "id": "x1pyUhsFEOza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develop a classification model for classifying the quality of an unseen wine, from the\n",
        "measured set of attributes, as good or bad. The threshold score for good wine is 7\n",
        "i.e. score of 7 or higher is good."
      ],
      "metadata": {
        "id": "Qf6TTKAlETBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Learn (via online-search and discussion) the meaning of the following terms.**"
      ],
      "metadata": {
        "id": "ioNxc3GaEWrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   True Positives (TP): Both actually and predicted are positive\n",
        "*   False Positives (FP): Actually is negative and predicted is positive\n",
        "*   True Negatives (TN): Both actually and predicted are negative\n",
        "*   False Negatives (FN): Actually is positive and predicted is negative\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DQWKK_MOEaTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Interpret the definition of each the following performance metrics and discuss its implication.**"
      ],
      "metadata": {
        "id": "AKXAWI4IIsU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Accuracy = (𝑇𝑃+𝑇𝑁)/(𝑇𝑃+𝐹𝑃+𝑇𝑁+𝐹𝑁)\n",
        "           A common metric used to evaluate the overall correctness. It measures the proportion of correctly classified instances (TP + TN) out of the total instances in the dataset (TP + FP + TN + FN). It shows how often your model is correct overall, but can be misleading when classes are imbalanced.\n",
        "*   Precision = 𝑇𝑃/(𝑇𝑃+𝐹𝑃)\n",
        "           A metric that focuses on the instances that the model predicted as positive (TP + FP). How many of the things your model said are positive are actually correct (TP). This make high precision means the model is good at avoiding false positives (FP), but it might miss some true positives (TP).\n",
        "*   Recall (or Sensitivity) = 𝑇𝑃/(𝑇𝑃+𝐹𝑁)\n",
        "           A metric that focuses on the instances that the model actually as positive (TP + FN). How many of the actual positive things (TP) your model found out of all the positive things (TP + FN). This make high recall indicates that the model is effective at finding most of the actual positive instances but it might predict more false positives (FP).\n",
        "*   F1 Score = 2/((1/Precision) +(1/Recall))\n",
        "           The harmonic mean of precision and recall. It provides a balance between these two metrics, taking into account both false positives (FP) and false negatives (FN). This making it useful for situations where you want to balance precision and recall. It's especially valuable when classes are imbalanced."
      ],
      "metadata": {
        "id": "WMdq_FzdOt5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Decide on how you will approach the problem**"
      ],
      "metadata": {
        "id": "GatHzU76O_HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red wines only"
      ],
      "metadata": {
        "id": "8FFknmqkvJcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Split the dataset into training set and testing set. For those who understand the usage of validation set, the testing set may be used as the validation set in this learning session.**"
      ],
      "metadata": {
        "id": "lFB-RWcSwU3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "nFzfLRc5xyWO",
        "outputId": "ab567982-3793-41da-b231-6430b79f1116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-27778b77-9bb1-4956-b4cc-c4fbc5875eae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-27778b77-9bb1-4956-b4cc-c4fbc5875eae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving winequality-red.csv to winequality-red.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "# load data\n",
        "df = pd.read_csv(io.StringIO(uploaded['winequality-red.csv'].decode('utf-8')), delimiter=';', quotechar='\"')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "hj4-w-iox6Be",
        "outputId": "560af46a-5201-4576-a816-3423c4000dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0               7.4             0.700         0.00             1.9      0.076   \n",
              "1               7.8             0.880         0.00             2.6      0.098   \n",
              "2               7.8             0.760         0.04             2.3      0.092   \n",
              "3              11.2             0.280         0.56             1.9      0.075   \n",
              "4               7.4             0.700         0.00             1.9      0.076   \n",
              "...             ...               ...          ...             ...        ...   \n",
              "1594            6.2             0.600         0.08             2.0      0.090   \n",
              "1595            5.9             0.550         0.10             2.2      0.062   \n",
              "1596            6.3             0.510         0.13             2.3      0.076   \n",
              "1597            5.9             0.645         0.12             2.0      0.075   \n",
              "1598            6.0             0.310         0.47             3.6      0.067   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
              "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
              "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
              "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "...                   ...                   ...      ...   ...        ...   \n",
              "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
              "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
              "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
              "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
              "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
              "\n",
              "      alcohol  quality  \n",
              "0         9.4        5  \n",
              "1         9.8        5  \n",
              "2         9.8        5  \n",
              "3         9.8        6  \n",
              "4         9.4        5  \n",
              "...       ...      ...  \n",
              "1594     10.5        5  \n",
              "1595     11.2        6  \n",
              "1596     11.0        6  \n",
              "1597     10.2        5  \n",
              "1598     11.0        6  \n",
              "\n",
              "[1599 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-119b1ca1-65c9-4a23-80fc-a0899f22730d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-119b1ca1-65c9-4a23-80fc-a0899f22730d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-119b1ca1-65c9-4a23-80fc-a0899f22730d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-119b1ca1-65c9-4a23-80fc-a0899f22730d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-08652e08-18db-4027-83af-16028e24002f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08652e08-18db-4027-83af-16028e24002f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-08652e08-18db-4027-83af-16028e24002f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset values to numeric\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "# add column quality_class with check quality when score more than or equal 7 is good otherwise bad\n",
        "df['quality_class'] = df['quality'].apply(lambda score: 'good' if score >= 7 else 'bad')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "CdUR_oWXysCC",
        "outputId": "57c3fcbe-73fe-4517-ac6d-c6b9dbe95a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0               7.4             0.700         0.00             1.9      0.076   \n",
              "1               7.8             0.880         0.00             2.6      0.098   \n",
              "2               7.8             0.760         0.04             2.3      0.092   \n",
              "3              11.2             0.280         0.56             1.9      0.075   \n",
              "4               7.4             0.700         0.00             1.9      0.076   \n",
              "...             ...               ...          ...             ...        ...   \n",
              "1594            6.2             0.600         0.08             2.0      0.090   \n",
              "1595            5.9             0.550         0.10             2.2      0.062   \n",
              "1596            6.3             0.510         0.13             2.3      0.076   \n",
              "1597            5.9             0.645         0.12             2.0      0.075   \n",
              "1598            6.0             0.310         0.47             3.6      0.067   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
              "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
              "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
              "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "...                   ...                   ...      ...   ...        ...   \n",
              "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
              "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
              "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
              "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
              "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
              "\n",
              "      alcohol  quality quality_class  \n",
              "0         9.4        5           bad  \n",
              "1         9.8        5           bad  \n",
              "2         9.8        5           bad  \n",
              "3         9.8        6           bad  \n",
              "4         9.4        5           bad  \n",
              "...       ...      ...           ...  \n",
              "1594     10.5        5           bad  \n",
              "1595     11.2        6           bad  \n",
              "1596     11.0        6           bad  \n",
              "1597     10.2        5           bad  \n",
              "1598     11.0        6           bad  \n",
              "\n",
              "[1599 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a0c9939-52be-48de-b914-11a37498ee46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "      <th>quality_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a0c9939-52be-48de-b914-11a37498ee46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a0c9939-52be-48de-b914-11a37498ee46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a0c9939-52be-48de-b914-11a37498ee46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7aee4cc8-e1f8-41e0-9114-f980abf4e718\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7aee4cc8-e1f8-41e0-9114-f980abf4e718')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7aee4cc8-e1f8-41e0-9114-f980abf4e718 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from yellowbrick.target.feature_correlation import feature_correlation\n",
        "\n",
        "X = df.drop(['quality', 'quality_class'], axis=1).values  # Features\n",
        "y = (df['quality_class'] == 'good').astype(int).values  # Convert to binary labels (0 or 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Split the dataset into training set and testing set\n",
        "# Correlation\n",
        "feature_labels = df.drop(['quality', 'quality_class'], axis=1).columns.tolist()\n",
        "visualizer = feature_correlation(X_train, y_train, labels=feature_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "v8ibXSC6zJoB",
        "outputId": "8e5a51e5-befc-4789-8298-03cdbcc84a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "positive = np.sum(y_train)  # Count of samples in the positive class\n",
        "negative = len(y_train) - positive  # Count of samples in the negative class\n",
        "positive_prop = positive / len(y_train)\n",
        "negative_prop = negative / len(y_train)\n",
        "print(f'Positive Class Proportion: {positive_prop:.4f}')\n",
        "print(f'Negative Class Proportion: {negative_prop:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTiPcZJZGxok",
        "outputId": "0c278de0-29b1-4016-9019-c7b3e36de875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Class Proportion: 0.1329\n",
            "Negative Class Proportion: 0.8671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Code a notebook to create a neural network model for classification. There will be a number of options to choose for the model**"
      ],
      "metadata": {
        "id": "L6TvkxA5z7IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The number of hidden layers is 1\n",
        "*   The number of neurons in each hidden layers is 32\n",
        "*   The activation functions are ReLU (Rectified Linear Unit) for the input and hidden layers, and Sigmoid for the output layer\n",
        "*   The Keras model to be used is Sequential\n",
        "*   The optimizer is Adam\n",
        "*   The number of epochs is 100 and the batch size is 64"
      ],
      "metadata": {
        "id": "5px5e7g86C2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Create a model with Sequential\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer\n",
        "model.add(Dense(32, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
        "# Compile\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Set up early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "# Train\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stopping])\n",
        "# Evaluate model on testing set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xxqnx1j0FoV",
        "outputId": "570ec25a-9774-47d4-818e-3be4b6277ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 2s 52ms/step - loss: 0.5713 - accuracy: 0.7742 - val_loss: 0.4490 - val_accuracy: 0.8828\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.4242 - accuracy: 0.8622 - val_loss: 0.3525 - val_accuracy: 0.8867\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3623 - accuracy: 0.8651 - val_loss: 0.3032 - val_accuracy: 0.8984\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3287 - accuracy: 0.8827 - val_loss: 0.2735 - val_accuracy: 0.8906\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3087 - accuracy: 0.8837 - val_loss: 0.2603 - val_accuracy: 0.8984\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2961 - accuracy: 0.8817 - val_loss: 0.2535 - val_accuracy: 0.8945\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2887 - accuracy: 0.8876 - val_loss: 0.2452 - val_accuracy: 0.8984\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2831 - accuracy: 0.8905 - val_loss: 0.2415 - val_accuracy: 0.8984\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2789 - accuracy: 0.8886 - val_loss: 0.2426 - val_accuracy: 0.8906\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2751 - accuracy: 0.8935 - val_loss: 0.2369 - val_accuracy: 0.8945\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2726 - accuracy: 0.8964 - val_loss: 0.2350 - val_accuracy: 0.9023\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2702 - accuracy: 0.8935 - val_loss: 0.2370 - val_accuracy: 0.9023\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2669 - accuracy: 0.8983 - val_loss: 0.2326 - val_accuracy: 0.8984\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2644 - accuracy: 0.8964 - val_loss: 0.2332 - val_accuracy: 0.9023\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2623 - accuracy: 0.8954 - val_loss: 0.2362 - val_accuracy: 0.9023\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2598 - accuracy: 0.8974 - val_loss: 0.2317 - val_accuracy: 0.8945\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2577 - accuracy: 0.8974 - val_loss: 0.2326 - val_accuracy: 0.8945\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2563 - accuracy: 0.8974 - val_loss: 0.2305 - val_accuracy: 0.8945\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2540 - accuracy: 0.8974 - val_loss: 0.2319 - val_accuracy: 0.9023\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2547 - accuracy: 0.8915 - val_loss: 0.2312 - val_accuracy: 0.9023\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2516 - accuracy: 0.9003 - val_loss: 0.2258 - val_accuracy: 0.8984\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2521 - accuracy: 0.8944 - val_loss: 0.2331 - val_accuracy: 0.9023\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2485 - accuracy: 0.8993 - val_loss: 0.2259 - val_accuracy: 0.8984\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2462 - accuracy: 0.8974 - val_loss: 0.2316 - val_accuracy: 0.8984\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2447 - accuracy: 0.8964 - val_loss: 0.2275 - val_accuracy: 0.8945\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2430 - accuracy: 0.8983 - val_loss: 0.2275 - val_accuracy: 0.8945\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2417 - accuracy: 0.8983 - val_loss: 0.2303 - val_accuracy: 0.9023\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2408 - accuracy: 0.8974 - val_loss: 0.2290 - val_accuracy: 0.8984\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.8974 - val_loss: 0.2240 - val_accuracy: 0.8906\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2375 - accuracy: 0.9022 - val_loss: 0.2252 - val_accuracy: 0.9023\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2369 - accuracy: 0.9013 - val_loss: 0.2234 - val_accuracy: 0.8945\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2357 - accuracy: 0.9013 - val_loss: 0.2243 - val_accuracy: 0.8906\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2335 - accuracy: 0.9013 - val_loss: 0.2271 - val_accuracy: 0.8984\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2314 - accuracy: 0.9052 - val_loss: 0.2247 - val_accuracy: 0.8906\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2306 - accuracy: 0.9032 - val_loss: 0.2231 - val_accuracy: 0.8867\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2299 - accuracy: 0.9022 - val_loss: 0.2254 - val_accuracy: 0.8867\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9052 - val_loss: 0.2237 - val_accuracy: 0.8828\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2256 - accuracy: 0.9071 - val_loss: 0.2223 - val_accuracy: 0.8867\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2247 - accuracy: 0.9062 - val_loss: 0.2239 - val_accuracy: 0.8828\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2231 - accuracy: 0.9071 - val_loss: 0.2204 - val_accuracy: 0.8945\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9042 - val_loss: 0.2218 - val_accuracy: 0.8828\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2205 - accuracy: 0.9091 - val_loss: 0.2228 - val_accuracy: 0.8828\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2183 - accuracy: 0.9110 - val_loss: 0.2201 - val_accuracy: 0.8867\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2185 - accuracy: 0.9101 - val_loss: 0.2221 - val_accuracy: 0.8828\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2160 - accuracy: 0.9091 - val_loss: 0.2205 - val_accuracy: 0.8828\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2152 - accuracy: 0.9091 - val_loss: 0.2228 - val_accuracy: 0.8828\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.9130 - val_loss: 0.2166 - val_accuracy: 0.8867\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9110 - val_loss: 0.2220 - val_accuracy: 0.8828\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2093 - accuracy: 0.9179 - val_loss: 0.2191 - val_accuracy: 0.8789\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.9150 - val_loss: 0.2177 - val_accuracy: 0.8867\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.9140 - val_loss: 0.2196 - val_accuracy: 0.8828\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2061 - accuracy: 0.9130 - val_loss: 0.2190 - val_accuracy: 0.8867\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2046 - accuracy: 0.9101 - val_loss: 0.2177 - val_accuracy: 0.8789\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2024 - accuracy: 0.9208 - val_loss: 0.2177 - val_accuracy: 0.8828\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9198 - val_loss: 0.2206 - val_accuracy: 0.8867\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9179 - val_loss: 0.2202 - val_accuracy: 0.8828\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1991 - accuracy: 0.9218 - val_loss: 0.2122 - val_accuracy: 0.8984\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1962 - accuracy: 0.9179 - val_loss: 0.2199 - val_accuracy: 0.8867\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1940 - accuracy: 0.9228 - val_loss: 0.2196 - val_accuracy: 0.8867\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1939 - accuracy: 0.9189 - val_loss: 0.2160 - val_accuracy: 0.8906\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1918 - accuracy: 0.9218 - val_loss: 0.2185 - val_accuracy: 0.8867\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1909 - accuracy: 0.9218 - val_loss: 0.2176 - val_accuracy: 0.8828\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1915 - accuracy: 0.9247 - val_loss: 0.2170 - val_accuracy: 0.8906\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.9238 - val_loss: 0.2148 - val_accuracy: 0.8828\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1855 - accuracy: 0.9247 - val_loss: 0.2206 - val_accuracy: 0.8867\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1831 - accuracy: 0.9277 - val_loss: 0.2184 - val_accuracy: 0.8867\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1834 - accuracy: 0.9316 - val_loss: 0.2111 - val_accuracy: 0.8984\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9257 - val_loss: 0.2245 - val_accuracy: 0.8867\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9326 - val_loss: 0.2128 - val_accuracy: 0.8984\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1775 - accuracy: 0.9296 - val_loss: 0.2182 - val_accuracy: 0.8867\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1769 - accuracy: 0.9326 - val_loss: 0.2217 - val_accuracy: 0.8867\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9335 - val_loss: 0.2182 - val_accuracy: 0.8789\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1748 - accuracy: 0.9365 - val_loss: 0.2135 - val_accuracy: 0.8984\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1727 - accuracy: 0.9296 - val_loss: 0.2148 - val_accuracy: 0.8984\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1720 - accuracy: 0.9345 - val_loss: 0.2257 - val_accuracy: 0.8906\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9335 - val_loss: 0.2159 - val_accuracy: 0.9062\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1702 - accuracy: 0.9335 - val_loss: 0.2148 - val_accuracy: 0.8906\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2418 - accuracy: 0.8875\n",
            "Test Loss: 0.2418, Test Accuracy: 0.8875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6) Code a cell to calculate performance metrics on the testing set results; accuracy, precision, recall, and f1. For this problem, it is natural to assume that “good” is positive.**"
      ],
      "metadata": {
        "id": "0jS5UjmdBQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary classes\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes)\n",
        "recall = recall_score(y_test, y_pred_classes)\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ_YA_brBVtr",
        "outputId": "da339fe1-bacd-4f39-dffa-9f44b084ebba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8875\n",
            "Precision: 0.6897\n",
            "Recall: 0.4255\n",
            "F1: 0.5263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7) Tune the model by iteratively adjusting the variables involved in the model development.**"
      ],
      "metadata": {
        "id": "EX9zOUnRSzQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's performance with learning rate\n",
        "*   Learning Rate : 0.001, 0.01, 0.1"
      ],
      "metadata": {
        "id": "zJs7P8pAtgS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "for learning_rate in learning_rates:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stopping], verbose=0)\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    precision = precision_score(y_test, y_pred_classes)\n",
        "    recall = recall_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes)\n",
        "    print(f'Learning Rate: {learning_rate}')\n",
        "    print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRC5A31mtgpU",
        "outputId": "25f09656-23d2-4447-fdc3-47ec77aebe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 3ms/step\n",
            "Learning Rate: 0.001\n",
            "Test Loss: 0.2396, Test Accuracy: 0.8969\n",
            "Accuracy: 0.8969, Precision: 0.7500, Recall: 0.4468, F1: 0.5600\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Learning Rate: 0.01\n",
            "Test Loss: 0.2455, Test Accuracy: 0.8719\n",
            "Accuracy: 0.8719, Precision: 0.7143, Recall: 0.2128, F1: 0.3279\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Learning Rate: 0.1\n",
            "Test Loss: 0.2820, Test Accuracy: 0.8594\n",
            "Accuracy: 0.8594, Precision: 0.5556, Recall: 0.2128, F1: 0.3077\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's performance with number of hidden layers and neurons in each layer\n",
        "*   Number of hidden layers : 1,2,3\n",
        "*   Neurons in each layer : 32,64,128\n",
        "\n"
      ],
      "metadata": {
        "id": "FhZgmHeQqn86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers_list = [1, 2, 3]\n",
        "num_neurons_list = [32, 64, 128]\n",
        "learning_rate = 0.001\n",
        "for num_layers in num_layers_list:\n",
        "    for num_neurons in num_neurons_list:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "        for _ in range(num_layers - 1):\n",
        "            model.add(Dense(num_neurons, activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stopping], verbose=0)\n",
        "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "        accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "        precision = precision_score(y_test, y_pred_classes)\n",
        "        recall = recall_score(y_test, y_pred_classes)\n",
        "        f1 = f1_score(y_test, y_pred_classes)\n",
        "        print(f\"Number of Hidden Layers: {num_layers}, Number of Neurons per Layer: {num_neurons}\")\n",
        "        print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\\n\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVKJPiAbS2Gr",
        "outputId": "3eb00378-8131-47da-e62f-46b26f534ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 1, Number of Neurons per Layer: 32\n",
            "Test Loss: 0.2588, Test Accuracy: 0.8750\n",
            "\n",
            "Accuracy: 0.8750, Precision: 0.6207, Recall: 0.3830, F1: 0.4737\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 1, Number of Neurons per Layer: 64\n",
            "Test Loss: 0.2621, Test Accuracy: 0.8781\n",
            "\n",
            "Accuracy: 0.8781, Precision: 0.6818, Recall: 0.3191, F1: 0.4348\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 1, Number of Neurons per Layer: 128\n",
            "Test Loss: 0.2528, Test Accuracy: 0.8719\n",
            "\n",
            "Accuracy: 0.8719, Precision: 0.6250, Recall: 0.3191, F1: 0.4225\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 2, Number of Neurons per Layer: 32\n",
            "Test Loss: 0.2444, Test Accuracy: 0.8750\n",
            "\n",
            "Accuracy: 0.8750, Precision: 0.6207, Recall: 0.3830, F1: 0.4737\n",
            "\n",
            "10/10 [==============================] - 0s 3ms/step\n",
            "Number of Hidden Layers: 2, Number of Neurons per Layer: 64\n",
            "Test Loss: 0.2388, Test Accuracy: 0.8875\n",
            "\n",
            "Accuracy: 0.8875, Precision: 0.7200, Recall: 0.3830, F1: 0.5000\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 2, Number of Neurons per Layer: 128\n",
            "Test Loss: 0.2403, Test Accuracy: 0.8906\n",
            "\n",
            "Accuracy: 0.8906, Precision: 0.7308, Recall: 0.4043, F1: 0.5205\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 3, Number of Neurons per Layer: 32\n",
            "Test Loss: 0.2405, Test Accuracy: 0.8781\n",
            "\n",
            "Accuracy: 0.8781, Precision: 0.6538, Recall: 0.3617, F1: 0.4658\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 3, Number of Neurons per Layer: 64\n",
            "Test Loss: 0.2471, Test Accuracy: 0.8719\n",
            "\n",
            "Accuracy: 0.8719, Precision: 0.6071, Recall: 0.3617, F1: 0.4533\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Number of Hidden Layers: 3, Number of Neurons per Layer: 128\n",
            "Test Loss: 0.2487, Test Accuracy: 0.8812\n",
            "\n",
            "Accuracy: 0.8812, Precision: 0.6667, Recall: 0.3830, F1: 0.4865\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's performance with activation functions\n",
        "*   Activation Functions : relu,sigmoid,tanh"
      ],
      "metadata": {
        "id": "TmccI4wvr6Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
        "learning_rate = 0.001\n",
        "hidden_layers = 2\n",
        "neurons_per_layer = 128\n",
        "for activation_function in activation_functions:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation=activation_function, input_shape=(X_train.shape[1],)))\n",
        "    for _ in range(hidden_layers - 1):\n",
        "            model.add(Dense(neurons_per_layer, activation=activation_function))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "    history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stopping], verbose=0)\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    precision = precision_score(y_test, y_pred_classes)\n",
        "    recall = recall_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes)\n",
        "    print(f'Activation Function: {activation_function}')\n",
        "    print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\\n')\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgW83zLor6a7",
        "outputId": "a04dac3a-7046-475f-bc8c-58858a9cac11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Activation Function: relu\n",
            "Test Loss: 0.2464, Test Accuracy: 0.8906\n",
            "\n",
            "Accuracy: 0.8906, Precision: 0.6667, Recall: 0.5106, F1: 0.5783\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Activation Function: sigmoid\n",
            "Test Loss: 0.2749, Test Accuracy: 0.8656\n",
            "\n",
            "Accuracy: 0.8656, Precision: 0.5833, Recall: 0.2979, F1: 0.3944\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Activation Function: tanh\n",
            "Test Loss: 0.2688, Test Accuracy: 0.8625\n",
            "\n",
            "Accuracy: 0.8625, Precision: 0.5556, Recall: 0.3191, F1: 0.4054\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model's performance with batch size and epochs\n",
        "*   Batch Size : 16,32,64\n",
        "*   Epochs : 50,100,150"
      ],
      "metadata": {
        "id": "5DUpCFY8uJgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [16, 32, 64]\n",
        "epochs_list = [50, 100, 150]\n",
        "learning_rate = 0.001\n",
        "hidden_layers = 2\n",
        "neurons_per_layer = 128\n",
        "activation_function = 'relu'\n",
        "for batch_size in batch_sizes:\n",
        "    for epochs in epochs_list:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, activation=activation_function, input_shape=(X_train.shape[1],)))\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            model.add(Dense(neurons_per_layer, activation=activation_function))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "        history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
        "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "        accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "        precision = precision_score(y_test, y_pred_classes)\n",
        "        recall = recall_score(y_test, y_pred_classes)\n",
        "        f1 = f1_score(y_test, y_pred_classes)\n",
        "        print(f'Batch Size: {batch_size}, Epochs: {epochs}')\n",
        "        print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\\n')\n",
        "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMxLq9YzuJu1",
        "outputId": "782328a7-50da-441f-b63a-3af02575e826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 16, Epochs: 50\n",
            "Test Loss: 0.2492, Test Accuracy: 0.8812\n",
            "\n",
            "Accuracy: 0.8812, Precision: 0.7647, Recall: 0.2766, F1: 0.4062\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 16, Epochs: 100\n",
            "Test Loss: 0.2269, Test Accuracy: 0.8875\n",
            "\n",
            "Accuracy: 0.8875, Precision: 0.7037, Recall: 0.4043, F1: 0.5135\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 16, Epochs: 150\n",
            "Test Loss: 0.2403, Test Accuracy: 0.8906\n",
            "\n",
            "Accuracy: 0.8906, Precision: 0.7500, Recall: 0.3830, F1: 0.5070\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 32, Epochs: 50\n",
            "Test Loss: 0.2474, Test Accuracy: 0.8812\n",
            "\n",
            "Accuracy: 0.8812, Precision: 0.7143, Recall: 0.3191, F1: 0.4412\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 32, Epochs: 100\n",
            "Test Loss: 0.2552, Test Accuracy: 0.8688\n",
            "\n",
            "Accuracy: 0.8688, Precision: 0.5926, Recall: 0.3404, F1: 0.4324\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 32, Epochs: 150\n",
            "Test Loss: 0.2374, Test Accuracy: 0.8875\n",
            "\n",
            "Accuracy: 0.8875, Precision: 0.7391, Recall: 0.3617, F1: 0.4857\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 64, Epochs: 50\n",
            "Test Loss: 0.2410, Test Accuracy: 0.8844\n",
            "\n",
            "Accuracy: 0.8844, Precision: 0.6923, Recall: 0.3830, F1: 0.4932\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 64, Epochs: 100\n",
            "Test Loss: 0.2447, Test Accuracy: 0.8781\n",
            "\n",
            "Accuracy: 0.8781, Precision: 0.6250, Recall: 0.4255, F1: 0.5063\n",
            "\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Batch Size: 64, Epochs: 150\n",
            "Test Loss: 0.2401, Test Accuracy: 0.8844\n",
            "\n",
            "Accuracy: 0.8844, Precision: 0.6389, Recall: 0.4894, F1: 0.5542\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best performance\n",
        "*   Learning Rate: 0.001\n",
        "*   Number of Hidden Layers and Neurons: 2 hidden layers and 128 neurons\n",
        "*   Activation Functions: 'relu'\n",
        "*   Batch Size and Epochs: batch size of 16 and training for 100 epochs"
      ],
      "metadata": {
        "id": "5pHUpB6p0ap7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "hidden_layers = 2\n",
        "neurons_per_layer = 128\n",
        "activation_function = 'relu'\n",
        "batch_size = 16\n",
        "epochs = 100\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=activation_function, input_shape=(X_train.shape[1],)))\n",
        "for _ in range(hidden_layers - 1):\n",
        "    model.add(Dense(neurons_per_layer, activation=activation_function))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OZMdRe32rJy",
        "outputId": "83961108-1d0f-499f-8891-8202a088fa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "64/64 [==============================] - 1s 6ms/step - loss: 0.3918 - accuracy: 0.8602 - val_loss: 0.2661 - val_accuracy: 0.9062\n",
            "Epoch 2/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8837 - val_loss: 0.2592 - val_accuracy: 0.8750\n",
            "Epoch 3/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8778 - val_loss: 0.2398 - val_accuracy: 0.8984\n",
            "Epoch 4/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8876 - val_loss: 0.2450 - val_accuracy: 0.8828\n",
            "Epoch 5/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8886 - val_loss: 0.2494 - val_accuracy: 0.8711\n",
            "Epoch 6/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8964 - val_loss: 0.2407 - val_accuracy: 0.8867\n",
            "Epoch 7/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8974 - val_loss: 0.2459 - val_accuracy: 0.8789\n",
            "Epoch 8/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2571 - accuracy: 0.8964 - val_loss: 0.2336 - val_accuracy: 0.8867\n",
            "Epoch 9/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.2497 - accuracy: 0.8983 - val_loss: 0.2389 - val_accuracy: 0.8789\n",
            "Epoch 10/100\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.2462 - accuracy: 0.9003 - val_loss: 0.2336 - val_accuracy: 0.8711\n",
            "Epoch 11/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2432 - accuracy: 0.9022 - val_loss: 0.2232 - val_accuracy: 0.8945\n",
            "Epoch 12/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2339 - accuracy: 0.9052 - val_loss: 0.2230 - val_accuracy: 0.8945\n",
            "Epoch 13/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2300 - accuracy: 0.9120 - val_loss: 0.2185 - val_accuracy: 0.8945\n",
            "Epoch 14/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2260 - accuracy: 0.9032 - val_loss: 0.2382 - val_accuracy: 0.8750\n",
            "Epoch 15/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2227 - accuracy: 0.9062 - val_loss: 0.2243 - val_accuracy: 0.8906\n",
            "Epoch 16/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2191 - accuracy: 0.9179 - val_loss: 0.2223 - val_accuracy: 0.8867\n",
            "Epoch 17/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9179 - val_loss: 0.2168 - val_accuracy: 0.8867\n",
            "Epoch 18/100\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.2058 - accuracy: 0.9238 - val_loss: 0.2178 - val_accuracy: 0.8906\n",
            "Epoch 19/100\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.2000 - accuracy: 0.9198 - val_loss: 0.2241 - val_accuracy: 0.8945\n",
            "Epoch 20/100\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.1974 - accuracy: 0.9238 - val_loss: 0.2230 - val_accuracy: 0.8867\n",
            "Epoch 21/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9296 - val_loss: 0.2163 - val_accuracy: 0.8945\n",
            "Epoch 22/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9286 - val_loss: 0.2332 - val_accuracy: 0.8906\n",
            "Epoch 23/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1911 - accuracy: 0.9277 - val_loss: 0.2193 - val_accuracy: 0.9023\n",
            "Epoch 24/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9326 - val_loss: 0.2287 - val_accuracy: 0.8945\n",
            "Epoch 25/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9326 - val_loss: 0.2302 - val_accuracy: 0.8984\n",
            "Epoch 26/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.9365 - val_loss: 0.2161 - val_accuracy: 0.9023\n",
            "Epoch 27/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1696 - accuracy: 0.9335 - val_loss: 0.2237 - val_accuracy: 0.8906\n",
            "Epoch 28/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9413 - val_loss: 0.2199 - val_accuracy: 0.9062\n",
            "Epoch 29/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9462 - val_loss: 0.2333 - val_accuracy: 0.8984\n",
            "Epoch 30/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.9413 - val_loss: 0.2212 - val_accuracy: 0.9102\n",
            "Epoch 31/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9521 - val_loss: 0.2301 - val_accuracy: 0.9102\n",
            "Epoch 32/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.9433 - val_loss: 0.2431 - val_accuracy: 0.8984\n",
            "Epoch 33/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9492 - val_loss: 0.2339 - val_accuracy: 0.9102\n",
            "Epoch 34/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9472 - val_loss: 0.2380 - val_accuracy: 0.9023\n",
            "Epoch 35/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.9589 - val_loss: 0.2339 - val_accuracy: 0.9102\n",
            "Epoch 36/100\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9521 - val_loss: 0.2361 - val_accuracy: 0.9102\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.8844\n",
            "Test Loss: 0.2561, Test Accuracy: 0.8844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes)\n",
        "recall = recall_score(y_test, y_pred_classes)\n",
        "f1 = f1_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH5MSuhgDewA",
        "outputId": "d2e961ea-612e-44a4-f9a0-8f87c6af96b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8844\n",
            "Precision: 0.8571\n",
            "Recall: 0.2553\n",
            "F1: 0.3934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8) Summarize in a notebook text cell about what you eventually make use of the performance metrics. What are the approaches that you found to help optimize the values of performance metrics altogether.**"
      ],
      "metadata": {
        "id": "zmHJDddx2dIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Proportion: This indicates the proportion of \"good\" wines in the dataset. In your case, about 13.29% of the wines are classified as \"good,\" while the remaining 86.71% are classified as \"bad\". This severe class imbalance could potentially impact the model's ability to accurately predict positive class.\n",
        "\n",
        "approaches used to optimize the model:\n",
        "*   Learning Rate\n",
        "*   Hidden Layers and Neurons\n",
        "*   Activation Function\n",
        "*   Batch Size and Epochs\n",
        "\n",
        "Before Tuning:\n",
        "*   Accuracy: A accuracy of 0.8875 indicates the overall correctness of the model's predictions. However, in an imbalanced dataset, accuracy can be misleading. For instance, if the model predominantly predicts the negative class, it can still achieve high accuracy due to the class imbalance.\n",
        "*   Precision: A precision of 0.6897 means that when the model predicts a wine as \"good,\" it is correct about 68.97% of the time. This indicates the model's ability to make accurate positive predictions.\n",
        "*   Recall: A recall of 0.4255 means that the model is able to identify about 42.55% of the actual \"good\" wines. This is a moderate level of recall and implies that the model is missing a significant number of \"good\" wines.\n",
        "*   F1-Score: The F1-score of 0.5263 is the harmonic mean of precision and recall. It balances the trade-off between false positives and false negatives. A value above 0.5 suggests a relatively good balance between precision and recall.\n",
        "\n",
        "After Tuning:\n",
        "*   Accuracy: A accuracy of 0.8844 is similar to accuracy before tuning. However, as mentioned earlier, accuracy alone might not provide a complete picture due to the class imbalance.\n",
        "*   Precision: A precision of 0.8571 indicates a significant improvement in the model's ability to accurately predict the positive class. When it predicts a wine as \"good,\" it is correct about 85.71% of the time.\n",
        "*   Recall: The recall has decreased to 0.2553. This means that the model is now able to identify only about 25.53% of the actual \"good\" wines. The increase in precision seems to have come at the cost of lower recall.\n",
        "*   F1: A F1-score of 0.3934 indicates a trade-off between precision and recall, as it's lower compared to the F1-score before tuning.\n",
        "\n",
        "The class imbalance in the dataset is likely impacting the model's performance. The high precision after tuning suggests that the model has improved in accurately predicting the positive class, but this improvement has come at the expense of recall.\n",
        "Depending on your business goals, you might need to decide whether precision or recall is more important. For instance, in a scenario where identifying \"good\" wines is critical , you might prioritize recall over precision.\n",
        "In conclusion, the choice of performance metrics and model evaluation depends on specific goals and the importance of accurately predicting each class in the context of wine quality classification."
      ],
      "metadata": {
        "id": "ShJyec612i77"
      }
    }
  ]
}